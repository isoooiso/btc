# analyze_predictions.py - –ê–ù–ê–õ–ò–ó –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô –ú–û–î–ï–õ–ï–ô
import torch
import joblib
import numpy as np
import pandas as pd
from data_loader import load_and_update_data
from features import (
    add_technical_indicators, filter_anomalies, add_onchain_features,
    add_macro_features, add_multiscale_features, add_fear_greed_index, 
    add_btc_dominance, add_google_trends, add_additional_macro, 
    add_correlations_and_external, add_temporal_features, add_fed_rate,
    create_dual_target
)
from config import *

print("="*80)
print("üîç –ê–ù–ê–õ–ò–ó –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô –ú–û–î–ï–õ–ï–ô")
print("="*80 + "\n")

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
df = load_and_update_data()
df = add_technical_indicators(df)
df = add_multiscale_features(df)
df = add_temporal_features(df)
df = filter_anomalies(df)
df = add_onchain_features(df)
df = add_macro_features(df)
df = add_fear_greed_index(df)
df = add_btc_dominance(df)
df = add_google_trends(df)
df = add_fed_rate(df)
df = add_additional_macro(df)
df = add_correlations_and_external(df)

# –¢–∞—Ä–≥–µ—Ç—ã
df = create_dual_target(df, short=FUTURE_TARGET_SHORT, long=FUTURE_TARGET_LONG)
df = df[(df['target_short'] != -1) & (df['target_long'] != -1)].copy()

# Split
train_end_idx = int(0.8 * len(df))
val_df = df.iloc[train_end_idx:].copy()

print(f"Validation –¥–∞–Ω–Ω—ã—Ö: {len(val_df)} —Å—Ç—Ä–æ–∫")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
try:
    scaler = joblib.load(SCALER_PATH)
    imputer = joblib.load('data/imputer.pkl')
    feature_cols = joblib.load(SELECTED_FEATURE_COLS_PATH)
    lgbm_model = joblib.load(LGBM_MODEL_PATH)
    
    print(f"–ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –§–∏—á–µ–π: {len(feature_cols)}\n")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ validation –¥–∞–Ω–Ω—ã—Ö
    X_val = val_df[feature_cols].copy()
    y_val = val_df['target_short'].values
    
    # –û—á–∏—Å—Ç–∫–∞
    for col in feature_cols:
        X_val[col] = X_val[col].replace([np.inf, -np.inf], np.nan)
    
    X_val_imp = imputer.transform(X_val)
    X_val_scaled = scaler.transform(X_val_imp)
    X_val_scaled = np.nan_to_num(X_val_scaled, nan=0.0)
    
    # LGBM –ø—Ä–æ–≥–Ω–æ–∑—ã
    lgbm_probs = lgbm_model.predict(X_val_scaled)
    lgbm_preds = (lgbm_probs > 0.5).astype(int)
    
    # –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞
    if os.path.exists('data/isotonic_calibrator.pkl'):
        calibrator = joblib.load('data/isotonic_calibrator.pkl')
        lgbm_probs_calibrated = calibrator.transform(lgbm_probs)
    else:
        lgbm_probs_calibrated = lgbm_probs
    
    # –ê–Ω–∞–ª–∏–∑
    print("="*80)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê LGBM –ü–†–û–ì–ù–û–ó–û–í –ù–ê VALIDATION:")
    print("="*80)
    print(f"\n–ë–µ–∑ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏:")
    print(f"  Min prob: {lgbm_probs.min():.4f}")
    print(f"  Max prob: {lgbm_probs.max():.4f}")
    print(f"  Mean prob: {lgbm_probs.mean():.4f}")
    print(f"  Median prob: {np.median(lgbm_probs):.4f}")
    print(f"  Std prob: {lgbm_probs.std():.4f}")
    
    print(f"\n–° –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π:")
    print(f"  Min prob: {lgbm_probs_calibrated.min():.4f}")
    print(f"  Max prob: {lgbm_probs_calibrated.max():.4f}")
    print(f"  Mean prob: {lgbm_probs_calibrated.mean():.4f}")
    print(f"  Median prob: {np.median(lgbm_probs_calibrated):.4f}")
    print(f"  Std prob: {lgbm_probs_calibrated.std():.4f}")
    
    # Accuracy
    from sklearn.metrics import accuracy_score
    acc_uncalib = accuracy_score(y_val, lgbm_preds)
    calibrated_preds = (lgbm_probs_calibrated > 0.5).astype(int)
    acc_calib = accuracy_score(y_val, calibrated_preds)
    
    print(f"\nAccuracy:")
    print(f"  –ë–µ–∑ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏: {acc_uncalib:.3f}")
    print(f"  –° –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π: {acc_calib:.3f}")
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
    print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ (–±–µ–∑ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏):")
    print(f"  –õ–û–ù–ì (prob > 0.5): {(lgbm_probs > 0.5).sum()} ({(lgbm_probs > 0.5).sum()/len(lgbm_probs)*100:.1f}%)")
    print(f"  –®–û–†–¢ (prob < 0.5): {(lgbm_probs < 0.5).sum()} ({(lgbm_probs < 0.5).sum()/len(lgbm_probs)*100:.1f}%)")
    
    print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ (—Å –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π):")
    print(f"  –õ–û–ù–ì (prob > 0.5): {(lgbm_probs_calibrated > 0.5).sum()} ({(lgbm_probs_calibrated > 0.5).sum()/len(lgbm_probs_calibrated)*100:.1f}%)")
    print(f"  –®–û–†–¢ (prob < 0.5): {(lgbm_probs_calibrated < 0.5).sum()} ({(lgbm_probs_calibrated < 0.5).sum()/len(lgbm_probs_calibrated)*100:.1f}%)")
    
    # –ü–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–æ–≥–Ω–æ–∑ (—Ç–µ–∫—É—â–∏–π)
    print("\n" + "="*80)
    print("–¢–ï–ö–£–©–ò–ô –ü–†–û–ì–ù–û–ó (–ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–æ–∫–∞ val):")
    print("="*80)
    print(f"LGBM prob (–±–µ–∑ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏): {lgbm_probs[-1]:.4f}")
    print(f"LGBM prob (—Å –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π): {lgbm_probs_calibrated[-1]:.4f}")
    print(f"–ü—Ä–æ–≥–Ω–æ–∑: {'–õ–û–ù–ì' if lgbm_probs_calibrated[-1] > 0.5 else '–®–û–†–¢'}")
    print(f"Actual target (–µ—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–µ–Ω): {y_val[-1]}")
    
    print("\n" + "="*80)
    print("–í–´–í–û–î:")
    print("="*80)
    if lgbm_probs_calibrated.mean() < 0.3:
        print("‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ê: –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞!")
        print("   –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å < 0.3 –æ–∑–Ω–∞—á–∞–µ—Ç —Å–∏–ª—å–Ω—ã–π bias –∫ –®–û–†–¢–£")
        print("   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –û—Ç–∫–ª—é—á–∏—Ç—å –∫–∞–ª–∏–±—Ä–æ–≤–∫—É –∏–ª–∏ –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å –Ω–∞ balanced –¥–∞–Ω–Ω—ã—Ö")
    elif lgbm_probs_calibrated.mean() > 0.7:
        print("‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ê: –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞ –≤ —Å—Ç–æ—Ä–æ–Ω—É –õ–û–ù–ì–ê!")
    else:
        print("‚úÖ –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤—ã–≥–ª—è–¥–∏—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ (mean ~0.5)")
        
except Exception as e:
    print(f"–û—à–∏–±–∫–∞: {e}")
    import traceback
    traceback.print_exc()

print("="*80)