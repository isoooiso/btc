# main.py
import os
import torch
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
import joblib
import warnings
import optuna
import lightgbm as lgb

# === –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–º–ø–æ—Ä—Ç Lightning ===
try:
    import lightning.pytorch as pl
    from lightning.pytorch import Trainer as _Trainer
    print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è lightning.pytorch")
except Exception:
    import pytorch_lightning as pl
    from pytorch_lightning import Trainer as _Trainer
    print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è pytorch_lightning")

from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer
from pytorch_forecasting.metrics import QuantileLoss
from pytorch_forecasting.data import GroupNormalizer

from data_loader import load_and_update_data
from features import (
    add_technical_indicators, filter_anomalies, add_onchain_features,
    add_macro_features, create_dual_target, create_regression_target,
    add_multiscale_features, add_fear_greed_index, add_btc_dominance,
    add_google_trends, add_additional_macro, add_correlations_and_external,
    add_temporal_features, add_fed_rate  # –ù–û–í–û–ï!
)
from model import train_lgbm, train_stacking, train_regression, calibrate_lgbm_probs
from predict import predict_ensemble
from config import *

warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

pl.seed_everything(42)

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def clean_and_normalize(train_df, val_df, feature_cols):
    """–û—á–∏—Å—Ç–∫–∞, –∏–º–ø—É—Ç–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    X_train = train_df[feature_cols].copy()
    X_val = val_df[feature_cols].copy()

    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∫–æ–ª–æ–Ω–æ–∫
    X_train = X_train.loc[:, ~X_train.columns.duplicated()]
    X_val = X_val.loc[:, ~X_val.columns.duplicated()]
    feature_cols = X_train.columns.tolist()

    # –ó–∞–º–µ–Ω–∞ inf –Ω–∞ NaN
    X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_val.replace([np.inf, -np.inf], np.nan, inplace=True)

    # –ò–º–ø—É—Ç–∞—Ü–∏—è
    imputer = SimpleImputer(strategy='median')
    X_train_imp = imputer.fit_transform(X_train)
    X_val_imp = imputer.transform(X_val)

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_imp)
    X_val_scaled = scaler.transform(X_val_imp)

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ NaN
    X_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0)
    X_val_scaled = np.nan_to_num(X_val_scaled, nan=0.0)

    return X_train_scaled, X_val_scaled, scaler, imputer, feature_cols


def tune_tft(training, train_dataloader, val_dataloader):
    """Hyperparameter tuning –¥–ª—è TFT —á–µ—Ä–µ–∑ Optuna"""
    def objective(trial):
        config = {
            "learning_rate": trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),
            "hidden_size": trial.suggest_int('hidden_size', 16, 128),
            "attention_head_size": trial.suggest_int('attention_head_size', 1, 4),
            "dropout": trial.suggest_float('dropout', 0.1, 0.3),
            "hidden_continuous_size": trial.suggest_int('hidden_continuous_size', 8, 64),
        }
        tft = TemporalFusionTransformer.from_dataset(
            training, **config, output_size=7, loss=QuantileLoss()
        )
        trainer = _Trainer(
            max_epochs=3,
            accelerator='gpu' if torch.cuda.is_available() else 'cpu',
            logger=False,
            enable_checkpointing=False,
            enable_progress_bar=False
        )
        trainer.fit(tft, train_dataloader, val_dataloader)
        val_loss = trainer.callback_metrics.get('val_loss', float('inf'))
        return val_loss

    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=5)
    return study.best_params


def tune_lgbm(X_train, y_train, X_val, y_val):
    """Hyperparameter tuning –¥–ª—è LightGBM —á–µ—Ä–µ–∑ Optuna"""
    def objective(trial):
        params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),
            'num_leaves': trial.suggest_int('num_leaves', 31, 256),
            'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
            'verbose': -1,
            'seed': 42,
            'device': 'cpu'
        }
        lgb_train = lgb.Dataset(X_train, label=y_train)
        lgb_val = lgb.Dataset(X_val, label=y_val)
        model = lgb.train(
            params, lgb_train, num_boost_round=1000,
            valid_sets=[lgb_val],
            callbacks=[lgb.early_stopping(50)]
        )
        pred = (model.predict(X_val) > 0.5).astype(int)
        return 1 - accuracy_score(y_val, pred)

    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=10)
    return study.best_params


def backtest_model(df, model, feature_cols, lookback=168, future=6):
    """–ü—Ä–æ—Å—Ç–æ–π –±—ç–∫—Ç–µ—Å—Ç –º–æ–¥–µ–ª–∏"""
    preds = []
    for i in range(lookback, len(df) - future, 24):
        window = df.iloc[i-lookback:i][feature_cols]
        pred = model.predict(window.mean().values.reshape(1, -1))[0] > 0.5
        actual = df['close'].iloc[i + future] > df['close'].iloc[i]
        preds.append((pred, actual))
    
    if len(preds) == 0:
        return 0.5
    
    acc = np.mean([p == a for p, a in preds])
    print(f"Backtest accuracy: {acc:.3f}")
    return acc


# ============================================================================
# MAIN PIPELINE
# ============================================================================

def main():
    print("="*80)
    print("üöÄ BTC –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä v2.3 - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø")
    print("   ‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω—ã –±–∞–≥–∏ (duplicate labels, calibration)")
    print("   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ (–ª–∞–≥–∏, rolling, momentum)")
    print("="*80 + "\n")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\n")

    # ========================================================================
    # 1. –ó–ê–ì–†–£–ó–ö–ê –ò –û–ë–û–ì–ê–©–ï–ù–ò–ï –î–ê–ù–ù–´–•
    # ========================================================================
    print("=" * 60)
    print("–≠–¢–ê–ü 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    print("=" * 60)
    
    df = load_and_update_data()
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(df)} —á–∞—Å–æ–≤—ã—Ö —Å–≤–µ—á–µ–π\n")
    
    # –ë–∞–∑–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    print("=" * 60)
    print("–≠–¢–ê–ü 2: –ë–∞–∑–æ–≤—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã")
    print("=" * 60)
    df = add_technical_indicators(df)
    
    # –ú—É–ª—å—Ç–∏–º–∞—Å—à—Ç–∞–±–Ω—ã–µ —Ñ–∏—á–∏ (4h, 1d)
    print("=" * 60)
    print("–≠–¢–ê–ü 3: –ú—É–ª—å—Ç–∏–º–∞—Å—à—Ç–∞–±–Ω—ã–µ —Ñ–∏—á–∏")
    print("=" * 60)
    df = add_multiscale_features(df)
    
    # === –ù–û–í–û–ï: –ü–†–û–î–í–ò–ù–£–¢–´–ï –í–†–ï–ú–ï–ù–ù–´–ï –§–ò–ß–ò ===
    print("=" * 60)
    print("–≠–¢–ê–ü 4: üåü –ü–†–û–î–í–ò–ù–£–¢–´–ï –í–†–ï–ú–ï–ù–ù–´–ï –§–ò–ß–ò (–ù–û–í–û–ï!)")
    print("=" * 60)
    df = add_temporal_features(df)
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π
    print("=" * 60)
    print("–≠–¢–ê–ü 5: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π")
    print("=" * 60)
    df = filter_anomalies(df)
    
    # On-chain –∏ –º–∞–∫—Ä–æ
    print("=" * 60)
    print("–≠–¢–ê–ü 6: On-chain –∏ –º–∞–∫—Ä–æ-–¥–∞–Ω–Ω—ã–µ")
    print("=" * 60)
    df = add_onchain_features(df)
    df = add_macro_features(df)
    df = add_fear_greed_index(df)
    df = add_btc_dominance(df)
    df = add_google_trends(df)
    df = add_fed_rate(df)
    df = add_additional_macro(df)
    df = add_correlations_and_external(df)

    # ========================================================================
    # 2. –°–û–ó–î–ê–ù–ò–ï –¢–ê–†–ì–ï–¢–û–í
    # ========================================================================
    print("\n" + "=" * 60)
    print("–≠–¢–ê–ü 7: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–æ–≤ (6h short, 24h long)")
    print("=" * 60)
    
    df = create_dual_target(df, short=FUTURE_TARGET_SHORT, long=FUTURE_TARGET_LONG)
    df = df[(df['target_short'] != -1) & (df['target_long'] != -1)].copy()
    print(f"–î–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ–ø–æ–ª–Ω—ã—Ö —Ç–∞—Ä–≥–µ—Ç–æ–≤: {len(df)}\n")

    # ========================================================================
    # 3. TRAIN/VAL SPLIT
    # ========================================================================
    print("=" * 60)
    print("–≠–¢–ê–ü 8: Train/Val split (80/20)")
    print("=" * 60)
    
    train_end_idx = int(0.8 * len(df))
    train_df = df.iloc[:train_end_idx].copy()
    val_df = df.iloc[train_end_idx:].copy()
    print(f"Train: {len(train_df)} | Val: {len(val_df)}\n")

    # ========================================================================
    # 4. FEATURE SELECTION
    # ========================================================================
    print("=" * 60)
    print("–≠–¢–ê–ü 9: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏—á–µ–π")
    print("=" * 60)
    
    feature_cols = [c for c in df.columns if c not in ['target_short', 'target_long', 'pct_change']]
    print(f"–í—Å–µ–≥–æ —Ñ–∏—á–µ–π: {len(feature_cols)}")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    X_train_scaled, X_val_scaled, scaler, imputer, feature_cols = clean_and_normalize(
        train_df, val_df, feature_cols
    )

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    os.makedirs('data', exist_ok=True)
    os.makedirs('models', exist_ok=True)
    joblib.dump(scaler, SCALER_PATH)
    joblib.dump(imputer, IMPUTER_PATH)
    joblib.dump(feature_cols, FEATURE_COLS_PATH)

    # –¢–∞—Ä–≥–µ—Ç—ã
    y_train_short = train_df['target_short'].values
    y_val_short = val_df['target_short'].values

    # ========================================================================
    # 5. –†–ï–ì–†–ï–°–°–ò–Ø –î–õ–Ø SHAP (feature importance)
    # ========================================================================
    print("\n" + "=" * 60)
    print("–≠–¢–ê–ü 10: –†–µ–≥—Ä–µ—Å—Å–∏—è –¥–ª—è SHAP –∞–Ω–∞–ª–∏–∑–∞")
    print("=" * 60)
    
    df_reg = create_regression_target(df.copy(), future=FUTURE_TARGET_SHORT)
    df_reg = df_reg[df_reg['pct_change'].notna()].copy()

    if len(df_reg) > 100:
        X_reg_train = X_train_scaled[:len(df_reg) - len(val_df)]
        y_reg_train = df_reg['pct_change'].iloc[:len(X_reg_train)].values
        X_reg_val = X_val_scaled[:len(df_reg) - len(X_reg_train)]
        y_reg_val = df_reg['pct_change'].iloc[len(X_reg_train):len(X_reg_train) + len(X_reg_val)].values

        if len(y_reg_train) > 0 and len(y_reg_val) > 0:
            train_regression(X_reg_train, y_reg_train, X_reg_val, y_reg_val, feature_cols)

    # Feature selection —á–µ—Ä–µ–∑ SHAP
    selected_features = feature_cols
    if os.path.exists(SHAP_VALUES_PATH):
        shap_values = joblib.load(SHAP_VALUES_PATH)
        shap_importance = np.abs(shap_values).mean(0)
        importance_df = pd.DataFrame({'feature': feature_cols, 'importance': shap_importance})
        importance_df = importance_df.sort_values('importance', ascending=False)
        
        # –£–±–∏—Ä–∞–µ–º bottom 20%
        threshold = importance_df['importance'].quantile(0.2)
        selected_features = importance_df[importance_df['importance'] > threshold]['feature'].tolist()
        print(f"\n‚úÖ Feature selection: –æ—Ç–æ–±—Ä–∞–Ω–æ {len(selected_features)} –∏–∑ {len(feature_cols)} —Ñ–∏—á–µ–π")
        
        joblib.dump(selected_features, SELECTED_FEATURE_COLS_PATH)

        # Re-normalize –Ω–∞ selected features
        train_df_selected = train_df[selected_features + ['target_short']]
        val_df_selected = val_df[selected_features + ['target_short']]

        X_train_scaled, X_val_scaled, scaler, imputer, selected_features = clean_and_normalize(
            train_df_selected, val_df_selected, selected_features
        )

        joblib.dump(scaler, SCALER_PATH)
        joblib.dump(imputer, IMPUTER_PATH)

        y_train_short = train_df_selected['target_short'].values
        y_val_short = val_df_selected['target_short'].values

    # ========================================================================
    # 6. HYPERPARAMETER TUNING
    # ========================================================================
    print("\n" + "=" * 60)
    print("–≠–¢–ê–ü 11: Hyperparameter Tuning (Optuna)")
    print("=" * 60)
    
    # TFT Tuning
    print("\nTuning TFT...")
    train_raw = train_df[selected_features].copy() if 'selected_features' in locals() else train_df[feature_cols].copy()
    val_raw = val_df[selected_features].copy() if 'selected_features' in locals() else val_df[feature_cols].copy()

    # === –ö–†–ò–¢–ò–ß–ù–û: –û—á–∏—Å—Ç–∫–∞ inf/nan –¥–ª—è TFT ===
    print("  –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç inf/nan...")
    train_raw = train_raw.replace([np.inf, -np.inf], np.nan)
    val_raw = val_raw.replace([np.inf, -np.inf], np.nan)
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –º–µ–¥–∏–∞–Ω–æ–π
    for col in train_raw.columns:
        if train_raw[col].isna().sum() > 0:
            median_val = train_raw[col].median()
            train_raw[col] = train_raw[col].fillna(median_val)
            val_raw[col] = val_raw[col].fillna(median_val)
    
    print(f"  ‚úÖ –î–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã: train={len(train_raw)}, val={len(val_raw)}")

    train_raw['target'] = y_train_short.astype(float)
    val_raw['target'] = y_val_short.astype(float)
    train_raw['time_idx'] = np.arange(len(train_raw))
    val_raw['time_idx'] = np.arange(len(train_raw), len(train_raw) + len(val_raw))
    train_raw['group'] = 0
    val_raw['group'] = 0

    full_df_tft = pd.concat([train_raw, val_raw], ignore_index=True)

    training = TimeSeriesDataSet(
        full_df_tft[lambda x: x.time_idx < len(train_raw)],
        time_idx="time_idx",
        target="target",
        group_ids=["group"],
        min_encoder_length=LOOKBACK//2,
        max_encoder_length=LOOKBACK,
        min_prediction_length=1,
        max_prediction_length=1,
        static_categoricals=[],
        static_reals=[],
        time_varying_known_categoricals=[],
        time_varying_known_reals=selected_features if 'selected_features' in locals() else feature_cols,
        time_varying_unknown_reals=["target"],
        target_normalizer=GroupNormalizer(groups=["group"], transformation="softplus"),
        add_relative_time_idx=True,
        add_target_scales=True,
        add_encoder_length=True,
    )

    validation = TimeSeriesDataSet.from_dataset(training, full_df_tft, predict=False, stop_randomization=True)

    train_dataloader = training.to_dataloader(train=True, batch_size=64, num_workers=0)
    val_dataloader = validation.to_dataloader(train=False, batch_size=64, num_workers=0)
    
    best_tft_params = tune_tft(training, train_dataloader, val_dataloader)
    print(f"‚úÖ Best TFT params: {best_tft_params}")

    # LGBM Tuning
    print("\nTuning LGBM...")
    best_lgbm_params = tune_lgbm(X_train_scaled, y_train_short, X_val_scaled, y_val_short)
    print(f"‚úÖ Best LGBM params: {best_lgbm_params}")

    # ========================================================================
    # 7. –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô
    # ========================================================================
    print("\n" + "=" * 60)
    print("–≠–¢–ê–ü 12: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")
    print("=" * 60)
    
    # TFT
    print("\n–û–±—É—á–µ–Ω–∏–µ TFT...")
    tft = TemporalFusionTransformer.from_dataset(
        training,
        **best_tft_params,
        output_size=7,
        loss=QuantileLoss(),
        log_interval=10,
        reduce_on_plateau_patience=3,
    )

    trainer = _Trainer(
        max_epochs=10,
        accelerator='gpu' if torch.cuda.is_available() else 'cpu',
        devices=1,
        enable_progress_bar=True,
        logger=False,
        enable_checkpointing=False,
    )

    trainer.fit(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)
    trainer.save_checkpoint(TFT_CHECKPOINT_PATH)
    joblib.dump(training, TFT_TRAINING_PATH)
    print("‚úÖ TFT –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")

    # TFT Predictions
    print("\n–ì–µ–Ω–µ—Ä–∞—Ü–∏—è TFT –ø—Ä–æ–≥–Ω–æ–∑–æ–≤...")
    tft.eval()
    with torch.no_grad():
        preds = tft.predict(val_dataloader, mode="quantiles")
        if isinstance(preds, torch.Tensor):
            preds = preds.cpu().numpy()
        tft_probs_val = preds[:, 0, 3]
        tft_probs_val = 1 / (1 + np.exp(-tft_probs_val))
        tft_probs_val = np.clip(tft_probs_val, 0.01, 0.99)

    # LGBM
    print("\n–û–±—É—á–µ–Ω–∏–µ LightGBM...")
    best_lgbm_params['objective'] = 'binary'
    best_lgbm_params['metric'] = 'binary_logloss'
    best_lgbm_params['verbose'] = -1
    best_lgbm_params['seed'] = 42
    best_lgbm_params['device'] = 'cpu'
    
    lgbm_model = train_lgbm(X_train_scaled, y_train_short, X_val_scaled, y_val_short, params=best_lgbm_params)
    lgbm_probs_val = calibrate_lgbm_probs(lgbm_model, X_train_scaled, y_train_short, X_val_scaled)

    # Stacking
    print("\n–û–±—É—á–µ–Ω–∏–µ Stacking...")
    train_stacking(
        tft_probs_val[:len(y_train_short)],
        lgbm_model.predict(X_train_scaled),
        y_train_short,
        tft_probs_val,
        lgbm_probs_val,
        y_val_short
    )

    # Regression (refit)
    print("\nRefit —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –Ω–∞ selected features...")
    df_reg = create_regression_target(df.copy(), future=FUTURE_TARGET_SHORT)
    df_reg = df_reg[df_reg['pct_change'].notna()].copy()

    if len(df_reg) > 100:
        X_reg_train = X_train_scaled[:len(df_reg) - len(val_df)]
        y_reg_train = df_reg['pct_change'].iloc[:len(X_reg_train)].values
        X_reg_val = X_val_scaled[:len(df_reg) - len(X_reg_train)]
        y_reg_val = df_reg['pct_change'].iloc[len(X_reg_train):len(X_reg_train) + len(X_reg_val)].values

        if len(y_reg_train) > 0 and len(y_reg_val) > 0:
            train_regression(X_reg_train, y_reg_train, X_reg_val, y_reg_val, 
                           selected_features if 'selected_features' in locals() else feature_cols)

    # ========================================================================
    # 8. BACKTESTING
    # ========================================================================
    print("\n" + "=" * 60)
    print("–≠–¢–ê–ü 13: Backtesting")
    print("=" * 60)
    
    backtest_acc = backtest_model(
        df, lgbm_model,
        selected_features if 'selected_features' in locals() else feature_cols
    )

    # ========================================================================
    # 9. –§–ò–ù–ê–õ–¨–ù–´–ô –ü–†–û–ì–ù–û–ó
    # ========================================================================
    print("\n" + "=" * 60)
    print("–≠–¢–ê–ü 14: –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑")
    print("=" * 60)
    
    df_pred = df.drop(columns=['target_short', 'target_long', 'pct_change'], errors='ignore')
    direction, confidence, pct_change, strength, prob_long = predict_ensemble(df_pred, device)

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "="*80)
    print("üéØ –ü–†–û–ì–ù–û–ó –ù–ê –°–õ–ï–î–£–Æ–©–ò–ï 6 –ß–ê–°–û–í:")
    print("="*80)
    print(f"   –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {direction}")
    print(f"   –ò–∑–º–µ–Ω–µ–Ω–∏–µ: {pct_change:+.2f}%")
    print(f"   –°–∏–ª–∞ –¥–≤–∏–∂–µ–Ω–∏—è: {strength}")
    print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1f}%")
    print(f"\nüìä 24H –¢–†–ï–ù–î: {'–í–í–ï–†–• ‚¨Ü' if prob_long > 50 else '–í–ù–ò–ó ‚¨á'} ({prob_long:.1f}%)")
    print("="*80 + "\n")

    # ========================================================================
    # 10. –°–û–•–†–ê–ù–ï–ù–ò–ï –ò–°–¢–û–†–ò–ò
    # ========================================================================
    history_path = 'data/forecast_history.csv'
    entry = {
        'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M'),
        'direction': direction,
        'pct_change': f"{pct_change:+.2f}%",
        'strength': strength,
        'confidence': f"{confidence:.1f}%",
        'backtest_acc': f"{backtest_acc:.3f}"
    }
    hist = pd.DataFrame([entry])
    if os.path.exists(history_path):
        hist = pd.concat([pd.read_csv(history_path), hist], ignore_index=True)
    hist.tail(50).to_csv(history_path, index=False)
    print(f"‚úÖ –ü—Ä–æ–≥–Ω–æ–∑ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {history_path}\n")

    print("="*80)
    print("üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
    print("="*80)


if __name__ == "__main__":
    main()