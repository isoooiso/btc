# online_learning.py - –°–ò–°–¢–ï–ú–ê –û–ù–õ–ê–ô–ù-–û–ë–£–ß–ï–ù–ò–Ø
import pandas as pd
import numpy as np
import joblib
import os
from datetime import datetime, timedelta
import lightgbm as lgb
from sklearn.metrics import accuracy_score
from config import *
from features import *

class OnlineLearner:
    """
    –°–∏—Å—Ç–µ–º–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    
    –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
    1. –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã
    2. –ö–æ–≥–¥–∞ –Ω–∞–∫–æ–ø–∏–ª–æ—Å—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 50+) ‚Üí –¥–æ–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
    3. –ò—Å–ø–æ–ª—å–∑—É–µ–º incremental learning –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –≤–µ—Ä—Å–∏–π –º–æ–¥–µ–ª–µ–π
    """
    
    def __init__(self, min_samples_for_retrain=50):
        self.min_samples = min_samples_for_retrain
        self.predictions_db = 'data/predictions_db.csv'
        self.retrain_log = 'data/retrain_log.csv'
    
    def should_retrain(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω—É–∂–Ω–æ –ª–∏ –¥–æ–æ–±—É—á–∞—Ç—å—Å—è"""
        if not os.path.exists(self.predictions_db):
            return False, 0
        
        df = pd.read_csv(self.predictions_db)
        checked = df[df['checked'] == True]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–∫–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        if os.path.exists(self.retrain_log):
            log = pd.read_csv(self.retrain_log)
            last_retrain = pd.to_datetime(log.iloc[-1]['timestamp'])
            new_data = checked[pd.to_datetime(checked['timestamp']) > last_retrain]
        else:
            new_data = checked
        
        return len(new_data) >= self.min_samples, len(new_data)
    
    def retrain_lgbm(self, new_data_df):
        """
        –î–æ–æ–±—É—á–µ–Ω–∏–µ LightGBM –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∫ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º—É —Å–µ—Ç—É
        –∏ –ø–µ—Ä–µ–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Å –Ω–æ–≤—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        """
        print("\nüîÑ –î–æ–æ–±—É—á–µ–Ω–∏–µ LightGBM...")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å –∏ –¥–∞–Ω–Ω—ã–µ
            lgbm_model = joblib.load(LGBM_MODEL_PATH)
            scaler = joblib.load(SCALER_PATH)
            imputer = joblib.load(IMPUTER_PATH)
            feature_cols = joblib.load(SELECTED_FEATURE_COLS_PATH if os.path.exists(SELECTED_FEATURE_COLS_PATH) else FEATURE_COLS_PATH)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
            from data_loader import load_and_update_data

            
            print("  –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
            df = load_and_update_data()
            df = add_technical_indicators(df)
            df = add_multiscale_features(df)
            df = add_temporal_features(df)
            df = filter_anomalies(df)
            df = add_onchain_features(df)
            df = add_macro_features(df)
            df = add_fear_greed_index(df)
            df = add_btc_dominance(df)
            df = add_google_trends(df)
            df = add_fed_rate(df)
            df = add_additional_macro(df)
            df = add_correlations_and_external(df)
            df = add_derivatives_features(df)
            df = create_dual_target(df, short=FUTURE_TARGET_SHORT, long=FUTURE_TARGET_LONG)
            df = df[(df['target_short'] != -1) & (df['target_long'] != -1)].copy()
            
            # –ù–æ–≤—ã–π split: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 90% –¥–ª—è train, 10% –¥–ª—è val
            # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—å—Å—è –Ω–∞ –±–æ–ª–µ–µ —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            split_idx = int(0.9 * len(df))
            train_df = df.iloc[:split_idx].copy()
            val_df = df.iloc[split_idx:].copy()
            
            print(f"  Train: {len(train_df)}, Val: {len(val_df)}")
            
            from preprocessing import transform_with_preprocessor

            X_train = train_df[feature_cols].copy()
            X_val = val_df[feature_cols].copy()

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–π imputer + scaler
            X_train_scaled = transform_with_preprocessor(X_train, imputer, scaler)
            X_val_scaled = transform_with_preprocessor(X_val, imputer, scaler)

            
            y_train = train_df['target_short'].values
            y_val = val_df['target_short'].values
            
            # –û–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            lgb_train = lgb.Dataset(X_train_scaled, label=y_train)
            lgb_val = lgb.Dataset(X_val_scaled, label=y_val, reference=lgb_train)
            
            params = {
                'objective': 'binary',
                'metric': 'binary_logloss',
                'learning_rate': 0.05,
                'num_leaves': 31,
                'feature_fraction': 0.9,
                'bagging_fraction': 0.8,
                'bagging_freq': 5,
                'verbose': -1,
                'seed': 42,
                'device': 'cpu'
            }
            
            print("  –û–±—É—á–µ–Ω–∏–µ...")
            model = lgb.train(
                params,
                lgb_train,
                num_boost_round=1000,
                valid_sets=[lgb_val],
                callbacks=[lgb.early_stopping(50)]
            )
            
            # –û—Ü–µ–Ω–∫–∞
            val_pred = (model.predict(X_val_scaled) > 0.5).astype(int)
            acc = accuracy_score(y_val, val_pred)
            
            print(f"  ‚úÖ –ù–æ–≤–∞—è Val Accuracy: {acc:.3f}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_path = f'data/lgbm_model_backup_{timestamp}.pkl'
            
            # –ë—ç–∫–∞–ø —Å—Ç–∞—Ä–æ–π –º–æ–¥–µ–ª–∏
            if os.path.exists(LGBM_MODEL_PATH):
                joblib.dump(lgbm_model, backup_path)
                print(f"  –°—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {backup_path}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
            joblib.dump(model, LGBM_MODEL_PATH)
            
            print("  ‚úÖ –ù–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
            
            return acc
            
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def log_retrain(self, accuracy, samples_used):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è"""
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'accuracy': accuracy,
            'samples_used': samples_used,
            'model_version': datetime.now().strftime('%Y%m%d_%H%M%S')
        }
        
        if os.path.exists(self.retrain_log):
            log_df = pd.read_csv(self.retrain_log)
            log_df = pd.concat([log_df, pd.DataFrame([log_entry])], ignore_index=True)
        else:
            log_df = pd.DataFrame([log_entry])
        
        log_df.to_csv(self.retrain_log, index=False)
        print(f"\n‚úÖ –î–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–æ!")
    
    def run(self):
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥: –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ"""
        print("="*80)
        print("ü§ñ –ü–†–û–í–ï–†–ö–ê –ù–ï–û–ë–•–û–î–ò–ú–û–°–¢–ò –î–û–û–ë–£–ß–ï–ù–ò–Ø")
        print("="*80)
        
        should_train, n_new = self.should_retrain()
        
        if not should_train:
            print(f"\n‚úÖ –î–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
            print(f"   –ù–æ–≤—ã—Ö –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤: {n_new}/{self.min_samples}")
            return False
        
        print(f"\nüîÑ –ù–∞–∫–æ–ø–ª–µ–Ω–æ {n_new} –Ω–æ–≤—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ ‚Üí –Ω–∞—á–∏–Ω–∞–µ–º –¥–æ–æ–±—É—á–µ–Ω–∏–µ!")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        df = pd.read_csv(self.predictions_db)
        checked = df[df['checked'] == True]
        
        if os.path.exists(self.retrain_log):
            log = pd.read_csv(self.retrain_log)
            last_retrain = pd.to_datetime(log.iloc[-1]['timestamp'])
            new_data = checked[pd.to_datetime(checked['timestamp']) > last_retrain]
        else:
            new_data = checked
        
        # –î–æ–æ–±—É—á–µ–Ω–∏–µ LGBM
        acc = self.retrain_lgbm(new_data)
        
        if acc is not None:
            self.log_retrain(acc, len(new_data))
            print("\n" + "="*80)
            print("‚úÖ –î–û–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
            print("="*80)
            return True
        else:
            print("\n" + "="*80)
            print("‚ùå –î–û–û–ë–£–ß–ï–ù–ò–ï –ù–ï –£–î–ê–õ–û–°–¨")
            print("="*80)
            return False


if __name__ == "__main__":
    learner = OnlineLearner(min_samples_for_retrain=50)
    learner.run()