import pandas as pd
import requests
from datetime import datetime, timedelta
import ta
from config import *
from pytrends.request import TrendReq
import numpy as np
from tenacity import retry, stop_after_attempt, wait_fixed
import joblib
import os
import yfinance as yf
from scipy.stats.mstats import winsorize
import warnings

warnings.filterwarnings('ignore')

# ============================================================================
# –®–ê–ì 1: –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –ö–†–ò–¢–ò–ß–ï–°–ö–ò–• –ë–ê–ì–û–í
# ============================================================================

# === 1. –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –ò–ù–î–ò–ö–ê–¢–û–†–´ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, —Ä–∞–±–æ—Ç–∞–µ—Ç) ===
def add_technical_indicators(df):
    """–ë–∞–∑–æ–≤—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"""
    df = df.copy()
    
    # RSI
    df['rsi'] = ta.momentum.RSIIndicator(df['close'], window=14).rsi()
    
    # MACD
    macd = ta.trend.MACD(df['close'])
    df['macd'] = macd.macd()
    df['macd_signal'] = macd.macd_signal()
    df['macd_diff'] = df['macd'] - df['macd_signal']
    
    # Moving Averages
    df['sma_20'] = ta.trend.SMAIndicator(df['close'], window=20).sma_indicator()
    df['ema_12'] = ta.trend.EMAIndicator(df['close'], window=12).ema_indicator()
    df['ema_26'] = ta.trend.EMAIndicator(df['close'], window=26).ema_indicator()
    
    # Bollinger Bands
    bollinger = ta.volatility.BollingerBands(df['close'], window=20, window_dev=2)
    df['bb_upper'] = bollinger.bollinger_hband()
    df['bb_lower'] = bollinger.bollinger_lband()
    df['bb_middle'] = bollinger.bollinger_mavg()
    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
    
    # Returns and Volatility
    df['return'] = df['close'].pct_change()
    df['volatility'] = df['return'].rolling(20).std()
    
    # Volume indicators
    df['volume_sma'] = df['volume'].rolling(20).mean()
    df['volume_ratio'] = df['volume'] / (df['volume_sma'] + 1e-8)
    
    # Cleanup
    if 'volume_sma20' in df.columns:
        df = df.drop(columns=['volume_sma20'])
    
    return df


# === 2. –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ê–ù–û–ú–ê–õ–ò–ô (—É–ª—É—á—à–µ–Ω–Ω–∞—è) ===
def filter_anomalies(df, winsor_limits=WINSOR_LIMITS):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å winsorization"""
    if len(df) == 0:
        return df
    
    print(f"–î–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞–Ω–æ–º–∞–ª–∏–π: {len(df)} —Å—Ç—Ä–æ–∫")
    df = df.copy()
    
    # –ó–∞–º–µ–Ω–∞ inf –Ω–∞ NaN
    df = df.replace([np.inf, -np.inf], np.nan)
    
    # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ NaN –º–µ–¥–∏–∞–Ω–æ–π
    df_num = df.select_dtypes(include=[np.number])
    for col in df_num.columns:
        if df[col].isna().sum() > 0:
            df[col] = df[col].fillna(df[col].median())
    
    # Winsorization –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    key_cols = ['return', 'volume_ratio', 'rsi', 'macd']
    key_cols = [col for col in key_cols if col in df.columns]
    
    for col in key_cols:
        if df[col].notna().sum() > 10:  # –ú–∏–Ω–∏–º—É–º 10 –∑–Ω–∞—á–µ–Ω–∏–π
            df[col] = winsorize(df[col].dropna(), limits=winsor_limits)
    
    print(f"–ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(df)} —Å—Ç—Ä–æ–∫ (outliers clipped)")
    return df


# === 3. ON-CHAIN (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, —Ä–∞–±–æ—Ç–∞–µ—Ç) ===
def add_onchain_features(df):
    """On-chain –º–µ—Ç—Ä–∏–∫–∏"""
    print("–î–æ–±–∞–≤–ª—è–µ–º on-chain...")
    index = df.index
    sopr = pd.Series(1.0, index=index, name='sopr')
    mvrv = pd.Series(1.0, index=index, name='mvrv')

    # CoinMetrics SOPR
    if COINMETRICS_API_KEY and COINMETRICS_API_KEY != "YOUR_COINMETRICS_KEY":
        try:
            start = (index[0] - timedelta(days=1)).strftime('%Y-%m-%d')
            end = index[-1].strftime('%Y-%m-%d')
            url = "https://api.coinmetrics.io/v4/timeseries/asset-metrics"
            params = {
                'api_key': COINMETRICS_API_KEY,
                'assets': 'btc',
                'metrics': 'sopr',
                'frequency': '1h',
                'start_time': start,
                'end_time': end
            }
            resp = requests.get(url, params=params, timeout=10)
            if resp.status_code == 200 and 'data' in resp.json():
                data = resp.json()['data']
                temp_df = pd.DataFrame(data)
                temp_df['time'] = pd.to_datetime(temp_df['time'])
                temp_df.set_index('time', inplace=True)
                sopr_series = temp_df['sopr'].astype(float).resample('1h').ffill()
                sopr = sopr_series.reindex(index, method='nearest').fillna(1.0)
        except Exception as e:
            print(f"SOPR –æ—à–∏–±–∫–∞: {e}")

    # MVRV —á–µ—Ä–µ–∑ CoinGecko
    try:
        start_ts = int((index[0] - timedelta(days=1)).timestamp())
        end_ts = int(index[-1].timestamp())
        url = f"https://api.coingecko.com/api/v3/coins/bitcoin/market_chart/range"
        params = {'vs_currency': 'usd', 'from': start_ts, 'to': end_ts}
        resp = requests.get(url, params=params, timeout=10)
        if resp.status_code == 200 and 'market_caps' in resp.json():
            caps = resp.json()['market_caps']
            temp_df = pd.DataFrame(caps, columns=['ts', 'cap'])
            temp_df['ts'] = pd.to_datetime(temp_df['ts'], unit='ms')
            temp_df.set_index('ts', inplace=True)
            mvrv_series = temp_df['cap'].resample('1h').ffill()
            mvrv = mvrv_series.reindex(index, method='nearest')
            mvrv = (mvrv / mvrv.mean()).fillna(1.0)
    except Exception as e:
        print(f"MVRV –æ—à–∏–±–∫–∞: {e}")

    df = df.copy()
    df['sopr'] = sopr
    df['mvrv'] = mvrv
    return df


# === 4. –ú–ê–ö–†–û (–ò–°–ü–†–ê–í–õ–ï–ù–û: S&P500 —á–µ—Ä–µ–∑ yfinance) ===
def add_macro_features(df):
    """–ú–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)"""
    if df.empty or len(df) < 2:
        return df
    
    print("–î–æ–±–∞–≤–ª—è–µ–º –º–∞–∫—Ä–æ...")
    index = df.index
    start = index[0].strftime('%Y-%m-%d')
    end = (index[-1] + timedelta(days=1)).strftime('%Y-%m-%d')
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
    df['dxy'] = 100.0
    df['vix'] = 20.0
    df['sp500'] = 0.0
    
    try:
        # === S&P500 —á–µ—Ä–µ–∑ yfinance (–ò–°–ü–†–ê–í–õ–ï–ù–û!) ===
        print("  –ó–∞–≥—Ä—É–∂–∞–µ–º S&P500...")
        sp500_data = yf.download('^GSPC', start=start, end=end, progress=False)
        if not sp500_data.empty and 'Close' in sp500_data.columns:
            sp500_hourly = sp500_data['Close'].resample('1h').ffill()
            sp500_reindexed = sp500_hourly.reindex(index, method='ffill')
            df['sp500'] = sp500_reindexed.fillna(method='ffill').fillna(4000)
            print(f"  S&P500 OK: {len(sp500_reindexed.dropna())} —Ç–æ—á–µ–∫")
        else:
            print("  S&P500: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
    except Exception as e:
        print(f"  S&P500 –æ—à–∏–±–∫–∞: {e}")
    
    try:
        # === VIX —á–µ—Ä–µ–∑ yfinance ===
        print("  –ó–∞–≥—Ä—É–∂–∞–µ–º VIX...")
        vix_data = yf.download('^VIX', start=start, end=end, progress=False)
        if not vix_data.empty and 'Close' in vix_data.columns:
            vix_hourly = vix_data['Close'].resample('1h').ffill()
            vix_reindexed = vix_hourly.reindex(index, method='ffill')
            df['vix'] = vix_reindexed.fillna(method='ffill').fillna(20)
            print(f"  VIX OK: {len(vix_reindexed.dropna())} —Ç–æ—á–µ–∫")
    except Exception as e:
        print(f"  VIX –æ—à–∏–±–∫–∞: {e}")
    
    try:
        # === DXY —á–µ—Ä–µ–∑ yfinance ===
        print("  –ó–∞–≥—Ä—É–∂–∞–µ–º DXY...")
        dxy_data = yf.download('DX-Y.NYB', start=start, end=end, progress=False)
        if not dxy_data.empty and 'Close' in dxy_data.columns:
            dxy_hourly = dxy_data['Close'].resample('1h').ffill()
            dxy_reindexed = dxy_hourly.reindex(index, method='ffill')
            df['dxy'] = dxy_reindexed.fillna(method='ffill').fillna(100)
            print(f"  DXY OK: {len(dxy_reindexed.dropna())} —Ç–æ—á–µ–∫")
    except Exception as e:
        print(f"  DXY –æ—à–∏–±–∫–∞: {e}")
    
    return df


# === 5. FEAR & GREED (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ===
@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))
def add_fear_greed_index(df):
    """Fear & Greed Index"""
    print("–î–æ–±–∞–≤–ª—è–µ–º Fear & Greed Index...")
    index = df.index
    fg_series = pd.Series(50.0, index=index, name='fear_greed')
    cache_path = 'data/fg_cache.pkl'

    try:
        if os.path.exists(cache_path):
            cached = joblib.load(cache_path)
            if cached.index[-1] >= index[-1]:
                fg_series = cached.reindex(index, method='nearest').fillna(50)
                print("  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω cache –¥–ª—è F&G")
                df['fear_greed'] = fg_series
                return df

        url = "https://api.alternative.me/fng/?limit=0"
        resp = requests.get(url, timeout=10).json()
        fg_data = resp['data']
        fg_df = pd.DataFrame(fg_data)
        fg_df['timestamp'] = pd.to_datetime(fg_df['timestamp'], unit='s')
        fg_df.set_index('timestamp', inplace=True)
        fg_df['value'] = fg_df['value'].astype(float)
        fg_series = fg_df['value'].reindex(index, method='nearest').fillna(method='ffill').fillna(50)
        joblib.dump(fg_series, cache_path)
        print("  F&G –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        print(f"  F&G –æ—à–∏–±–∫–∞: {e}. Fallback –Ω–∞ 50")

    df['fear_greed'] = fg_series
    return df


# === 6. BTC DOMINANCE (–ò–°–ü–†–ê–í–õ–ï–ù–û: —É–±—Ä–∞–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã) ===
@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))
def add_btc_dominance(df):
    """BTC Dominance (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)"""
    print("–î–æ–±–∞–≤–ª—è–µ–º BTC Dominance...")
    index = df.index
    dom_series = pd.Series(50.0, index=index, name='btc_dominance')
    cache_path = 'data/dom_cache.pkl'

    try:
        if os.path.exists(cache_path):
            cached = joblib.load(cache_path)
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ –∫—ç—à–∞ –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
            cached = cached[~cached.index.duplicated(keep='last')]
            dom_series = cached.reindex(index, method='nearest').fillna(50)
            print("  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω cache –¥–ª—è Dominance")
        else:
            start_ts = int(index[0].timestamp())
            end_ts = int(index[-1].timestamp())
            url_btc = f"https://api.coingecko.com/api/v3/coins/bitcoin/market_chart/range?vs_currency=usd&from={start_ts}&to={end_ts}"
            
            resp_btc = requests.get(url_btc, timeout=10).json()
            if 'market_caps' in resp_btc:
                btc_cap = pd.DataFrame(resp_btc['market_caps'], columns=['ts', 'cap'])
                btc_cap['ts'] = pd.to_datetime(btc_cap['ts'], unit='ms')
                
                # === –ê–ì–†–ï–°–°–ò–í–ù–ê–Ø –æ—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ ===
                btc_cap = btc_cap.drop_duplicates(subset='ts', keep='last')
                btc_cap = btc_cap.set_index('ts')
                
                # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–Ω–¥–µ–∫—Å–∞ –ø–æ—Å–ª–µ set_index
                btc_cap = btc_cap[~btc_cap.index.duplicated(keep='last')]
                
                btc_cap_series = btc_cap['cap'].resample('1h').ffill()
                
                # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
                btc_cap_series = btc_cap_series[~btc_cap_series.index.duplicated(keep='last')]
                
                dom_series = btc_cap_series.reindex(index, method='ffill').fillna(50.0)

                joblib.dump(dom_series, cache_path)
                print("  Dominance –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        print(f"  Dominance –æ—à–∏–±–∫–∞: {e}. Fallback –Ω–∞ 50")

    df['btc_dominance'] = dom_series
    return df


# === 7. GOOGLE TRENDS (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ===
@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))
def add_google_trends(df):
    """Google Trends"""
    print("–î–æ–±–∞–≤–ª—è–µ–º Google Trends...")
    index = df.index
    trends_series = pd.Series(50.0, index=index, name='google_trends')
    cache_path = 'data/trends_cache.pkl'

    try:
        if os.path.exists(cache_path):
            cached = joblib.load(cache_path)
            if cached.index[-1] >= index[-1]:
                trends_series = cached.reindex(index, method='nearest').fillna(50)
                print("  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω cache –¥–ª—è Trends")
                df['google_trends'] = trends_series
                return df

        pytrends = TrendReq(hl='en-US', tz=360)
        kw_list = ["bitcoin"]
        timeframe = f"{(index[0] - pd.Timedelta(days=7)).strftime('%Y-%m-%d')} {index[-1].strftime('%Y-%m-%d')}"
        pytrends.build_payload(kw_list, cat=0, timeframe=timeframe, geo='', gprop='')
        trends_df = pytrends.interest_over_time()
        if 'bitcoin' in trends_df.columns:
            trends_df = trends_df['bitcoin'].resample('1h').ffill()
            trends_series = trends_df.reindex(index, method='nearest').fillna(50)
            joblib.dump(trends_series, cache_path)
            print("  Trends –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        print(f"  Trends –æ—à–∏–±–∫–∞: {e}. Fallback –Ω–∞ 50")
    
    df['google_trends'] = trends_series
    return df


@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))
def add_fed_rate(df):
    """Fed Funds Rate (–ò–°–ü–†–ê–í–õ–ï–ù–û: —É–±—Ä–∞–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–Ω–¥–µ–∫—Å–∞)"""
    print("–î–æ–±–∞–≤–ª—è–µ–º Fed Funds Rate...")
    index = df.index
    rate_series = pd.Series(5.0, index=index, name='fed_rate')
    cache_path = 'data/fed_cache.pkl'

    try:
        if os.path.exists(cache_path):
            cached = joblib.load(cache_path)
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –∫—ç—à–µ
            cached = cached[~cached.index.duplicated(keep='last')]
            rate_series = cached.reindex(index, method='ffill').fillna(5.0)
            print("  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω cache –¥–ª—è Fed Rate")
        else:
            url = "https://api.stlouisfed.org/fred/series/observations"
            params = {
                'series_id': 'FEDFUNDS',
                'api_key': FRED_API_KEY,
                'file_type': 'json',
                'observation_start': index[0].strftime('%Y-%m-%d'),
                'observation_end': index[-1].strftime('%Y-%m-%d')
            }
            resp = requests.get(url, params=params, timeout=10).json()
            if 'observations' in resp:
                data = [
                    {'date': obs['date'], 'value': float(obs['value'])}
                    for obs in resp['observations'] if obs['value'] != '.'
                ]
                fed_df = pd.DataFrame(data)
                fed_df['date'] = pd.to_datetime(fed_df['date'])

                # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–µ–π
                fed_df = fed_df.drop_duplicates(subset='date', keep='last')
                fed_df = fed_df.set_index('date')
                fed_df = fed_df[~fed_df.index.duplicated(keep='last')]

                hourly = fed_df['value'].resample('1h').ffill()
                hourly = hourly[~hourly.index.duplicated(keep='last')]

                rate_series = hourly.reindex(index, method='ffill').fillna(5.0)
                joblib.dump(rate_series, cache_path)
                print("  Fed Rate –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        print(f"  Fed Rate –æ—à–∏–±–∫–∞: {e}. Fallback –Ω–∞ 5.0")

    df['fed_rate'] = rate_series
    return df



# === 9. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ú–ê–ö–†–û (–ò–°–ü–†–ê–í–õ–ï–ù–û) ===
@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))
def add_additional_macro(df):
    """Unemployment –∏ CPI (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)"""
    print("–î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞–∫—Ä–æ (unemployment, inflation)...")
    index = df.index
    unrate_series = pd.Series(4.0, index=index, name='unemployment_rate')
    cpi_series = pd.Series(3.0, index=index, name='inflation_cpi')
    cache_path_un = 'data/unrate_cache.pkl'
    cache_path_cpi = 'data/cpi_cache.pkl'

    try:
        # Unemployment
        if os.path.exists(cache_path_un):
            cached = joblib.load(cache_path_un)
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ –∫—ç—à–∞
            cached = cached[~cached.index.duplicated(keep='last')]
            unrate_series = cached.reindex(index, method='ffill').fillna(4.0)
            print("  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω cache –¥–ª—è Unemployment")
        else:
            url = "https://api.stlouisfed.org/fred/series/observations"
            params = {
                'series_id': 'UNRATE',
                'api_key': FRED_API_KEY,
                'file_type': 'json',
                'observation_start': index[0].strftime('%Y-%m-%d'),
                'observation_end': index[-1].strftime('%Y-%m-%d')
            }
            resp = requests.get(url, params=params, timeout=10).json()
            if 'observations' in resp:
                data = [{'date': obs['date'], 'value': float(obs['value'])} 
                        for obs in resp['observations'] if obs['value'] != '.']
                df_un = pd.DataFrame(data)
                df_un['date'] = pd.to_datetime(df_un['date'])
                
                # === –ê–ì–†–ï–°–°–ò–í–ù–ê–Ø –æ—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ ===
                df_un = df_un.drop_duplicates(subset='date', keep='last')
                df_un = df_un.set_index('date')
                df_un = df_un[~df_un.index.duplicated(keep='last')]
                
                unrate_series = df_un['value'].resample('1h').ffill()
                unrate_series = unrate_series[~unrate_series.index.duplicated(keep='last')]
                unrate_series = unrate_series.reindex(index, method='ffill').fillna(4.0)
                
                joblib.dump(unrate_series, cache_path_un)
                print("  Unemployment –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")

        # CPI
        if os.path.exists(cache_path_cpi):
            cached = joblib.load(cache_path_cpi)
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ –∫—ç—à–∞
            cached = cached[~cached.index.duplicated(keep='last')]
            cpi_series = cached.reindex(index, method='ffill').fillna(3.0)
            print("  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω cache –¥–ª—è CPI")
        else:
            params['series_id'] = 'CPIAUCSL'
            resp = requests.get(url, params=params, timeout=10).json()
            if 'observations' in resp:
                data = [{'date': obs['date'], 'value': float(obs['value'])} 
                        for obs in resp['observations'] if obs['value'] != '.']
                df_cpi = pd.DataFrame(data)
                df_cpi['date'] = pd.to_datetime(df_cpi['date'])
                
                # === –ê–ì–†–ï–°–°–ò–í–ù–ê–Ø –æ—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ ===
                df_cpi = df_cpi.drop_duplicates(subset='date', keep='last')
                df_cpi = df_cpi.set_index('date')
                df_cpi = df_cpi[~df_cpi.index.duplicated(keep='last')]
                
                cpi_series = df_cpi['value'].pct_change(12)
                cpi_series = cpi_series.resample('1h').ffill()
                cpi_series = cpi_series[~cpi_series.index.duplicated(keep='last')]
                cpi_series = cpi_series.reindex(index, method='ffill').fillna(3.0) * 100
                
                joblib.dump(cpi_series, cache_path_cpi)
                print("  CPI –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        print(f"  –î–æ–ø. –º–∞–∫—Ä–æ –æ—à–∏–±–∫–∞: {e}. Fallback")

    df['unemployment_rate'] = unrate_series
    df['inflation_cpi'] = cpi_series
    return df


# === 10. –ö–û–†–†–ï–õ–Ø–¶–ò–ò –ò –í–ù–ï–®–ù–ò–ï –ê–ö–¢–ò–í–´ ===
def add_correlations_and_external(df):
    """–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å –¥—Ä—É–≥–∏–º–∏ –∞–∫—Ç–∏–≤–∞–º–∏"""
    print("–î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∏ –≤–Ω–µ—à–Ω–∏–µ —Ü–µ–Ω—ã (Nasdaq, Gold, ETH/BTC)...")
    index = df.index
    start = index[0].strftime('%Y-%m-%d')
    end = (index[-1] + timedelta(days=1)).strftime('%Y-%m-%d')
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    df['nasdaq_close'] = 0.0
    df['gold_close'] = 0.0
    df['eth_btc_ratio'] = 0.05
    df['btc_sp500_corr'] = 0.0
    df['btc_nasdaq_corr'] = 0.0

    try:
        # Nasdaq
        nasdaq = yf.download('^IXIC', start=start, end=end, progress=False)
        if not nasdaq.empty and 'Close' in nasdaq.columns:
            nasdaq_hourly = nasdaq['Close'].resample('1h').ffill()
            df['nasdaq_close'] = nasdaq_hourly.reindex(index, method='ffill').fillna(0)
            print("  Nasdaq –∑–∞–≥—Ä—É–∂–µ–Ω")

        # Gold
        gold = yf.download('GC=F', start=start, end=end, progress=False)
        if not gold.empty and 'Close' in gold.columns:
            gold_hourly = gold['Close'].resample('1h').ffill()
            df['gold_close'] = gold_hourly.reindex(index, method='ffill').fillna(0)
            print("  Gold –∑–∞–≥—Ä—É–∂–µ–Ω")

        # Rolling correlations
        if 'sp500' in df.columns and df['sp500'].notna().sum() > 20:
            df['btc_sp500_corr'] = df['close'].rolling(20).corr(df['sp500']).fillna(0)
        
        if df['nasdaq_close'].notna().sum() > 20:
            df['btc_nasdaq_corr'] = df['close'].rolling(20).corr(df['nasdaq_close']).fillna(0)

        # ETH/BTC ratio
        start_ts = int(index[0].timestamp())
        end_ts = int(index[-1].timestamp())
        url_eth = f"https://api.coingecko.com/api/v3/coins/ethereum/market_chart/range?vs_currency=btc&from={start_ts}&to={end_ts}"
        resp_eth = requests.get(url_eth, timeout=10).json()
        if 'prices' in resp_eth:
            eth_df = pd.DataFrame(resp_eth['prices'], columns=['ts', 'price'])
            eth_df['ts'] = pd.to_datetime(eth_df['ts'], unit='ms')
            eth_df.set_index('ts', inplace=True)
            df['eth_btc_ratio'] = eth_df['price'].resample('1h').ffill().reindex(index, method='ffill').fillna(0.05)
            print("  ETH/BTC –∑–∞–≥—Ä—É–∂–µ–Ω")

    except Exception as e:
        print(f"  –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –æ—à–∏–±–∫–∞: {e}. Fallback")

    return df


# ============================================================================
# –®–ê–ì 2: –ü–†–û–î–í–ò–ù–£–¢–´–ï –í–†–ï–ú–ï–ù–ù–´–ï –§–ò–ß–ò (–ù–û–í–û–ï!)
# ============================================================================

def add_temporal_features(df):
    """
    –ö–†–ò–¢–ò–ß–ù–´–ï –í–†–ï–ú–ï–ù–ù–´–ï –§–ò–ß–ò –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
    –≠—Ç–æ —Å–∞–º—ã–π –≤–∞–∂–Ω—ã–π –∞–ø–≥—Ä–µ–π–¥!
    """
    print("\nüöÄ –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏...")
    df = df.copy()
    
    # === 1. –õ–ê–ì–ò –¶–ï–ù–´ (—Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ!) ===
    print("  ‚Üí –°–æ–∑–¥–∞—ë–º –ª–∞–≥–∏ —Ü–µ–Ω—ã –∏ returns...")
    for lag in LAG_PERIODS:
        df[f'close_lag_{lag}h'] = df['close'].shift(lag)
        df[f'return_lag_{lag}h'] = df['return'].shift(lag)
        df[f'volume_lag_{lag}h'] = df['volume'].shift(lag)
    
    # === 2. ROLLING STATISTICS ===
    print("  ‚Üí –í—ã—á–∏—Å–ª—è–µ–º rolling —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...")
    for window in ROLLING_WINDOWS:
        # Price rolling
        df[f'close_mean_{window}h'] = df['close'].rolling(window).mean()
        df[f'close_std_{window}h'] = df['close'].rolling(window).std()
        df[f'close_min_{window}h'] = df['close'].rolling(window).min()
        df[f'close_max_{window}h'] = df['close'].rolling(window).max()
        
        # Distance from extremes
        df[f'dist_from_high_{window}h'] = (df['close'] - df[f'close_max_{window}h']) / df['close']
        df[f'dist_from_low_{window}h'] = (df['close'] - df[f'close_min_{window}h']) / df['close']
        
        # Volume rolling
        df[f'volume_mean_{window}h'] = df['volume'].rolling(window).mean()
        df[f'volume_std_{window}h'] = df['volume'].rolling(window).std()
        
        # Volatility rolling
        df[f'volatility_{window}h'] = df['return'].rolling(window).std()
    
    # === 3. MOMENTUM INDICATORS ===
    print("  ‚Üí –î–æ–±–∞–≤–ª—è–µ–º momentum –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã...")
    df['momentum_1h'] = df['close'].pct_change(1)
    df['momentum_3h'] = df['close'].pct_change(3)
    df['momentum_6h'] = df['close'].pct_change(6)
    df['momentum_12h'] = df['close'].pct_change(12)
    df['momentum_24h'] = df['close'].pct_change(24)
    df['momentum_48h'] = df['close'].pct_change(48)
    df['momentum_7d'] = df['close'].pct_change(168)
    
    # === 4. RATE OF CHANGE (ROC) ===
    print("  ‚Üí –í—ã—á–∏—Å–ª—è–µ–º rate of change...")
    for period in [3, 6, 12, 24, 72]:
        df[f'roc_{period}h'] = (df['close'] - df['close'].shift(period)) / df['close'].shift(period) * 100
    
    # === 5. –¶–ò–ö–õ–ò–ß–ï–°–ö–ò–ï –í–†–ï–ú–ï–ù–ù–´–ï –§–ò–ß–ò (–≤–∞–∂–Ω–æ –¥–ª—è TFT!) ===
    print("  ‚Üí –°–æ–∑–¥–∞—ë–º —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏...")
    df['hour'] = df.index.hour
    df['day_of_week'] = df.index.dayofweek
    df['day_of_month'] = df.index.day
    df['month'] = df.index.month
    df['week_of_year'] = df.index.isocalendar().week
    
    # –°–∏–Ω—É—Å–æ–∏–¥–∞–ª—å–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ (–ª—É—á—à–µ –¥–ª—è ML!)
    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
    df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
    df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
    
    # === 6. ACCELERATION (—É—Å–∫–æ—Ä–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã) ===
    print("  ‚Üí –í—ã—á–∏—Å–ª—è–µ–º —É—Å–∫–æ—Ä–µ–Ω–∏–µ...")
    df['acceleration_1h'] = df['momentum_1h'].diff()
    df['acceleration_6h'] = df['momentum_6h'].diff()
    df['acceleration_24h'] = df['momentum_24h'].diff()
    
    # === 7. RELATIVE STRENGTH ===
    print("  ‚Üí –°—á–∏—Ç–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é —Å–∏–ª—É...")
    for window in [6, 12, 24]:
        df[f'relative_strength_{window}h'] = df['close'] / df[f'close_mean_{window}h']
    
    # === 8. VOLATILITY REGIME ===
    print("  ‚Üí –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏...")
    vol_median = df['volatility'].rolling(168).median()  # –º–µ–¥–∏–∞–Ω–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –∑–∞ –Ω–µ–¥–µ–ª—é
    df['vol_regime'] = (df['volatility'] / vol_median).fillna(1.0)
    df['high_vol_regime'] = (df['vol_regime'] > 1.5).astype(int)
    
    # === 9. TREND STRENGTH ===
    print("  ‚Üí –í—ã—á–∏—Å–ª—è–µ–º —Å–∏–ª—É —Ç—Ä–µ–Ω–¥–∞...")
    for window in [24, 72, 168]:
        # Linear regression slope
        rolling_slope = df['close'].rolling(window).apply(
            lambda x: np.polyfit(np.arange(len(x)), x, 1)[0] if len(x) == window else np.nan,
            raw=True
        )
        df[f'trend_strength_{window}h'] = rolling_slope
    
    # === 10. CANDLE PATTERNS (–±–∞–∑–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–≤–µ—á–µ–π) ===
    print("  ‚Üí –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–≤–µ—á–µ–π...")
    df['body'] = df['close'] - df['open']
    df['body_pct'] = df['body'] / df['open'] * 100
    df['upper_shadow'] = df['high'] - df[['close', 'open']].max(axis=1)
    df['lower_shadow'] = df[['close', 'open']].min(axis=1) - df['low']
    df['is_bullish'] = (df['close'] > df['open']).astype(int)
    df['is_doji'] = (abs(df['body_pct']) < 0.1).astype(int)
    
    # === 11. VOLUME ANALYSIS ===
    print("  ‚Üí –£–≥–ª—É–±–ª–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–±—ä—ë–º–∞...")
    df['volume_price_trend'] = df['volume'] * df['return']
    df['obv'] = (df['volume'] * np.sign(df['return'])).cumsum()  # On-Balance Volume
    df['obv_ema'] = df['obv'].ewm(span=20).mean()
    
    # === 12. SUPPORT/RESISTANCE LEVELS ===
    print("  ‚Üí –í—ã—á–∏—Å–ª—è–µ–º —É—Ä–æ–≤–Ω–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏/—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è...")
    for window in [24, 168]:
        df[f'support_{window}h'] = df['low'].rolling(window).min()
        df[f'resistance_{window}h'] = df['high'].rolling(window).max()
        df[f'price_position_{window}h'] = (df['close'] - df[f'support_{window}h']) / \
                                           (df[f'resistance_{window}h'] - df[f'support_{window}h'] + 1e-8)
    
    # === –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–ß–ò–°–¢–ö–ê: —É–±–∏—Ä–∞–µ–º inf –∏ –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ NaN ===
    print("  ‚Üí –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ inf/nan...")
    df = df.replace([np.inf, -np.inf], np.nan)
    
    # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á–µ–π
    temporal_features = [c for c in df.columns if c not in ['open', 'high', 'low', 'close', 'volume']]
    print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(temporal_features)} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∏—á–µ–π!\n")
    
    return df

# === 11. –î–ï–†–ò–í–ê–¢–ò–í–´: FUNDING + OPEN INTEREST ===
def add_derivatives_features(df):
    """–ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –±–ª–æ–∫ –¥–µ—Ä–∏–≤–∞—Ç–∏–≤–æ–≤ (funding + OI)."""
    print("–î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ä–∏–≤–∞—Ç–∏–≤—ã (funding, open interest)...")
    df = df.copy()
    index = df.index

    import requests
    import time

    # ---------- FUNDING RATE ----------
    funding_series = pd.Series(0.0, index=index, name='funding_rate')
    funding_cache_path = 'data/funding_cache.pkl'

    try:
        if os.path.exists(funding_cache_path):
            cached = joblib.load(funding_cache_path)
            cached = cached[~cached.index.duplicated(keep='last')]
            funding_series = cached.reindex(index, method='ffill').fillna(0.0)
            print("  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω cache –¥–ª—è funding")
        else:
            print("  –ó–∞–≥—Ä—É–∂–∞–µ–º funding rate —Å Binance...")
            base_url = "https://fapi.binance.com"
            endpoint = "/fapi/v1/fundingRate"
            symbol = "BTCUSDT"

            start_ts = int((index[0] - pd.Timedelta(days=10)).timestamp() * 1000)
            end_ts = int((index[-1] + pd.Timedelta(days=1)).timestamp() * 1000)

            all_rows = []
            cur_start = start_ts
            limit = 1000

            while cur_start < end_ts:
                params = {
                    "symbol": symbol,
                    "startTime": cur_start,
                    "limit": limit,
                }
                resp = requests.get(base_url + endpoint, params=params, timeout=10)
                resp.raise_for_status()
                data = resp.json()
                if not data:
                    break

                all_rows.extend(data)
                last_time = int(data[-1]['fundingTime'])
                cur_start = last_time + 1

                time.sleep(0.1)
                if len(data) < limit:
                    break

            if all_rows:
                f_df = pd.DataFrame(all_rows)
                f_df['fundingTime'] = pd.to_datetime(f_df['fundingTime'], unit='ms')
                f_df.set_index('fundingTime', inplace=True)
                f_df['fundingRate'] = f_df['fundingRate'].astype(float)

                f_hourly = f_df['fundingRate'].resample('1h').ffill()
                f_hourly = f_hourly[~f_hourly.index.duplicated(keep='last')]
                funding_series = f_hourly.reindex(index, method='ffill').fillna(0.0)

                joblib.dump(funding_series, funding_cache_path)
                print(f"  Funding –∑–∞–≥—Ä—É–∂–µ–Ω: {len(f_hourly.dropna())} —Ç–æ—á–µ–∫")

    except Exception as e:
        print(f"  Funding –æ—à–∏–±–∫–∞: {e}. Fallback –Ω–∞ 0.0")

    df['funding_rate'] = funding_series

    # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏
    df['funding_rate_abs'] = df['funding_rate'].abs()
    df['funding_rate_rolling_24h'] = df['funding_rate'].rolling(24).mean()
    df['funding_rate_change_8h'] = df['funding_rate'].diff(8)

    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ –ø–æ funding
    # z-score –Ω–∞ –æ–∫–Ω–µ 7 –¥–Ω–µ–π
    roll_window = 24 * 7
    roll_mean = df['funding_rate'].rolling(roll_window).mean()
    roll_std = df['funding_rate'].rolling(roll_window).std()
    df['funding_zscore'] = (df['funding_rate'] - roll_mean) / (roll_std + 1e-8)

    # –†–µ–∂–∏–º funding: -1 / 0 / +1
    df['funding_regime'] = 0
    df.loc[df['funding_rate'] > 0.0001, 'funding_regime'] = 1
    df.loc[df['funding_rate'] < -0.0001, 'funding_regime'] = -1

    # –ë—ã–ª –ª–∏ flip –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
    df['funding_flip_24h'] = (
        np.sign(df['funding_rate']).diff(24).fillna(0).ne(0).astype(int)
    )

    # ---------- OPEN INTEREST ----------
    oi_series = pd.Series(np.nan, index=index, name='open_interest')
    oi_cache_path = 'data/oi_cache.pkl'

    try:
        if os.path.exists(oi_cache_path):
            cached = joblib.load(oi_cache_path)
            cached = cached[~cached.index.duplicated(keep='last')]
            oi_series = cached.reindex(index, method='ffill')
            print("  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω cache –¥–ª—è open interest")
        else:
            print("  –ó–∞–≥—Ä—É–∂–∞–µ–º open interest —Å Binance...")
            base_url = "https://fapi.binance.com"
            endpoint = "/futures/data/openInterestHist"
            symbol = "BTCUSDT"

            start_ts = int((index[0] - pd.Timedelta(days=30)).timestamp() * 1000)
            end_ts = int((index[-1] + pd.Timedelta(days=1)).timestamp() * 1000)

            all_rows = []
            cur_start = start_ts
            limit = 500

            while cur_start < end_ts:
                params = {
                    "symbol": symbol,
                    "period": "1h",
                    "limit": limit,
                    "startTime": cur_start,
                    "endTime": end_ts,
                }
                resp = requests.get(base_url + endpoint, params=params, timeout=10)
                if resp.status_code == 400:
                    # –£ Binance —á–∞—Å—Ç–æ 400, –µ—Å–ª–∏ –¥–∏–∞–ø–∞–∑–æ–Ω —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π –∏–ª–∏ –Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –∑–∞–ø—Ä–æ—Å
                    print("  Open Interest 400 –æ—à–∏–±–∫–∞, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–æ–ø—ã—Ç–∫–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                    break

                resp.raise_for_status()
                data = resp.json()
                if not data:
                    break

                all_rows.extend(data)
                last_time = int(data[-1]['timestamp'])
                cur_start = last_time + 1

                time.sleep(0.1)
                if len(data) < limit:
                    break

            if all_rows:
                oi_df = pd.DataFrame(all_rows)
                oi_df['timestamp'] = pd.to_datetime(oi_df['timestamp'], unit='ms')
                oi_df.set_index('timestamp', inplace=True)
                oi_df['sumOpenInterest'] = oi_df['sumOpenInterest'].astype(float)

                oi_hourly = oi_df['sumOpenInterest'].resample('1h').ffill()
                oi_hourly = oi_hourly[~oi_hourly.index.duplicated(keep='last')]
                oi_series = oi_hourly.reindex(index, method='ffill')

                joblib.dump(oi_series, oi_cache_path)
                print(f"  Open Interest –∑–∞–≥—Ä—É–∂–µ–Ω: {len(oi_hourly.dropna())} —Ç–æ—á–µ–∫")
    except Exception as e:
        print(f"  Open Interest –æ—à–∏–±–∫–∞: {e}. Fallback")

    # –î–∞–∂–µ –µ—Å–ª–∏ OI –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å, —Ñ–∏—á–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤–∞–ª–∏–¥–Ω—ã–º–∏
    df['open_interest'] = (
        oi_series.fillna(method='ffill').fillna(method='bfill').fillna(0.0)
    )

    # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏ –ø–æ OI
    df['open_interest_norm'] = df['open_interest'] / (
        df['open_interest'].rolling(24 * 30).mean() + 1e-8
    )
    df['open_interest_change_24h'] = df['open_interest'].diff(24)

    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ –ø–æ OI
    oi_roll_mean_7d = df['open_interest'].rolling(24 * 7).mean()
    oi_roll_std_7d = df['open_interest'].rolling(24 * 7).std()
    df['oi_zscore_7d'] = (df['open_interest'] - oi_roll_mean_7d) / (oi_roll_std_7d + 1e-8)

    df['oi_change_7d'] = df['open_interest'].diff(24 * 7)

    # –û—Ç–Ω–æ—à–µ–Ω–∏–µ OI –∫ –æ–±—ä—ë–º—É (–ø–µ—Ä–µ–≥—Ä–µ—Ç–æ—Å—Ç—å)
    if 'volume' in df.columns:
        vol_roll = df['volume'].rolling(24).sum()
        df['oi_volume_ratio'] = df['open_interest'] / (vol_roll + 1e-8)
    else:
        df['oi_volume_ratio'] = 0.0

    # Crowded long / crowded short
    # –í—ã—Å–æ–∫–∏–π funding + –≤—ã—Å–æ–∫–∏–π OI => crowded long
    # –ù–∏–∑–∫–∏–π funding + –≤—ã—Å–æ–∫–∏–π OI => crowded short
    df['crowded_long_score'] = (
        df['funding_zscore'].clip(lower=0) * df['oi_zscore_7d'].clip(lower=0)
    )
    df['crowded_short_score'] = (
        (-df['funding_zscore']).clip(lower=0) * df['oi_zscore_7d'].clip(lower=0)
    )

    return df

# === 12. LIVE ORDERBOOK FEATURES (–¢–û–õ–¨–ö–û –î–õ–Ø ONLINE-–ü–†–û–ì–ù–û–ó–û–í) ===
@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))
def add_orderbook_features_live(df, symbol="BTCUSDT", depth_limit=50, use_futures=True):
    """
    –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ–∏—á–µ–π –∏–∑ –¢–ï–ö–£–©–ï–ì–û –æ—Ä–¥–µ—Ä–±—É–∫–∞ Binance.

    –í–ê–ñ–ù–û:
    - –†–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤ online-—Ä–µ–∂–∏–º–µ (auto_predict_loop), –≥–¥–µ –Ω–∞–º –Ω—É–∂–µ–Ω –ø—Ä–æ–≥–Ω–æ–∑
      –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–≤–µ—á–µ.
    - –î–ª—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–≥–æ backtest —á–µ—Å—Ç–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ –Ω–µ–ª—å–∑—è, –ø–æ—ç—Ç–æ–º—É
      –≤ backtest –º—ã –≠–¢–û –ù–ï –í–´–ó–´–í–ê–ï–ú.

    –§–∏—á–∏:
    - ob_bid_volume_sum
    - ob_ask_volume_sum
    - ob_imbalance = (bid - ask) / (bid + ask)
    - ob_top_bid, ob_top_ask, ob_spread
    - ob_weighted_mid_price
    """
    print("–î–æ–±–∞–≤–ª—è–µ–º LIVE orderbook —Ñ–∏—á–∏...")
    df = df.copy()
    index = df.index

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–µ—Ñ–æ–ª—Ç–∞–º–∏
    df['ob_bid_volume_sum'] = 0.0
    df['ob_ask_volume_sum'] = 0.0
    df['ob_imbalance'] = 0.0
    df['ob_top_bid'] = df['close']
    df['ob_top_ask'] = df['close']
    df['ob_spread'] = 0.0
    df['ob_weighted_mid_price'] = df['close']

    if df.empty:
        return df

    # –†–∞–±–æ—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ç–æ—á–∫–æ–π (–ø–æ—Å–ª–µ–¥–Ω–∏–π timestamp)
    last_ts = index[-1]

    try:
        import requests
        base_url = "https://fapi.binance.com" if use_futures else "https://api.binance.com"
        endpoint = "/fapi/v1/depth" if use_futures else "/api/v3/depth"

        params = {
            "symbol": symbol,
            "limit": depth_limit,
        }

        resp = requests.get(base_url + endpoint, params=params, timeout=10)
        resp.raise_for_status()
        orderbook = resp.json()

        bids = orderbook.get("bids", [])
        asks = orderbook.get("asks", [])

        # bids/asks: [ [price, qty], ... ]
        bid_prices = np.array([float(b[0]) for b in bids], dtype=float)
        bid_qty = np.array([float(b[1]) for b in bids], dtype=float)

        ask_prices = np.array([float(a[0]) for a in asks], dtype=float)
        ask_qty = np.array([float(a[1]) for a in asks], dtype=float)

        bid_vol_sum = bid_qty.sum()
        ask_vol_sum = ask_qty.sum()

        if bid_vol_sum + ask_vol_sum > 0:
            imbalance = (bid_vol_sum - ask_vol_sum) / (bid_vol_sum + ask_vol_sum)
        else:
            imbalance = 0.0

        top_bid = bid_prices[0] if len(bid_prices) > 0 else df.loc[last_ts, 'close']
        top_ask = ask_prices[0] if len(ask_prices) > 0 else df.loc[last_ts, 'close']
        spread = top_ask - top_bid

        # –ü—Ä–æ—Å—Ç–æ–π –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π mid price
        if bid_vol_sum > 0 and ask_vol_sum > 0:
            weighted_bid = (bid_prices * bid_qty).sum() / bid_vol_sum
            weighted_ask = (ask_prices * ask_qty).sum() / ask_vol_sum
            weighted_mid = 0.5 * (weighted_bid + weighted_ask)
        else:
            weighted_mid = df.loc[last_ts, 'close']

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –≤ –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É
        df.loc[last_ts, 'ob_bid_volume_sum'] = bid_vol_sum
        df.loc[last_ts, 'ob_ask_volume_sum'] = ask_vol_sum
        df.loc[last_ts, 'ob_imbalance'] = imbalance
        df.loc[last_ts, 'ob_top_bid'] = top_bid
        df.loc[last_ts, 'ob_top_ask'] = top_ask
        df.loc[last_ts, 'ob_spread'] = spread
        df.loc[last_ts, 'ob_weighted_mid_price'] = weighted_mid

        print("  LIVE orderbook —Ñ–∏—á–∏ —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω—ã")

    except Exception as e:
        print(f"  Orderbook –æ—à–∏–±–∫–∞: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")

    return df


# === –ú–£–õ–¨–¢–ò–ú–ê–°–®–¢–ê–ë–ù–´–ï –§–ò–ß–ò ===
def add_multiscale_features(df_1h):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ–∏—á–µ–π —Å —Ä–∞–∑–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ (1h, 4h, 12h, 1d, 3d)."""
    print("–î–æ–±–∞–≤–ª—è–µ–º –º—É–ª—å—Ç–∏–º–∞—Å—à—Ç–∞–±–Ω—ã–µ —Ñ–∏—á–∏ (4h, 12h, 1d, 3d)...")
    df_1h = df_1h.copy()

    # --- 4h ---
    df_4h = df_1h.resample('4h').agg({
        'open': 'first', 'high': 'max', 'low': 'min',
        'close': 'last', 'volume': 'sum'
    })
    df_4h = add_technical_indicators(df_4h)
    df_4h = df_4h.add_suffix('_4h')
    df_4h = df_4h.reindex(df_1h.index, method='ffill')

    # --- 12h ---
    df_12h = df_1h.resample('12h').agg({
        'open': 'first', 'high': 'max', 'low': 'min',
        'close': 'last', 'volume': 'sum'
    })
    df_12h = add_technical_indicators(df_12h)
    df_12h = df_12h.add_suffix('_12h')
    df_12h = df_12h.reindex(df_1h.index, method='ffill')

    # --- 1d ---
    df_1d = df_1h.resample('1d').agg({
        'open': 'first', 'high': 'max', 'low': 'min',
        'close': 'last', 'volume': 'sum'
    })
    df_1d = add_technical_indicators(df_1d)
    df_1d = df_1d.add_suffix('_1d')
    df_1d = df_1d.reindex(df_1h.index, method='ffill')

    # --- 3d ---
    df_3d = df_1h.resample('3d').agg({
        'open': 'first', 'high': 'max', 'low': 'min',
        'close': 'last', 'volume': 'sum'
    })
    df_3d = add_technical_indicators(df_3d)
    df_3d = df_3d.add_suffix('_3d')
    df_3d = df_3d.reindex(df_1h.index, method='ffill')

    # –°—à–∏–≤–∞–µ–º –≤—Å—ë
    df = pd.concat([df_1h, df_4h, df_12h, df_1d, df_3d], axis=1)
    print("  –ú—É–ª—å—Ç–∏–º–∞—Å—à—Ç–∞–±–Ω—ã–µ —Ñ–∏—á–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã\n")
    return df



# === –¢–ê–†–ì–ï–¢–´ ===
def create_dual_target(df, short=FUTURE_TARGET_SHORT, long=FUTURE_TARGET_LONG):
    """–°–æ–∑–¥–∞–Ω–∏–µ –¥–≤—É—Ö —Ç–∞—Ä–≥–µ—Ç–æ–≤: –∫–æ—Ä–æ—Ç–∫–∏–π (6h) –∏ –¥–ª–∏–Ω–Ω—ã–π (24h)"""
    df = df.copy()
    if len(df) < long + 1:
        df['target_short'] = -1
        df['target_long'] = -1
        return df

    future_short = df['close'].shift(-short)
    future_long = df['close'].shift(-long)

    df['target_short'] = (future_short > df['close']).astype(int)
    df['target_long'] = (future_long > df['close']).astype(int)

    df.loc[df['target_short'].isna(), 'target_short'] = -1
    df.loc[df['target_long'].isna(), 'target_long'] = -1
    
    return df


def create_regression_target(df, future=FUTURE_TARGET_SHORT):
    """–†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç: % –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã + vol-–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç."""
    df = df.copy()
    if len(df) < future + 1:
        df['pct_change'] = np.nan
        df['realized_vol_24h'] = np.nan
        df['pct_change_vol_norm'] = np.nan
        return df

    # –ë–∞–∑–æ–≤—ã–π —Ç–∞—Ä–≥–µ—Ç: % –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã –≤–ø–µ—Ä—ë–¥ –Ω–∞ future —á–∞—Å–æ–≤
    df['future_close'] = df['close'].shift(-future)
    df['pct_change'] = (df['future_close'] - df['close']) / df['close'] * 100.0
    df.drop('future_close', axis=1, inplace=True, errors='ignore')

    # –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –∑–∞ 24 —á–∞—Å–∞ (std —Ä–µ—Ç—ë—Ä–Ω–æ–≤ –≤ %, rolling 24 –±–∞—Ä–∞)
    if 'return' not in df.columns:
        df['return'] = df['close'].pct_change()

    df['realized_vol_24h'] = df['return'].rolling(24).std() * 100.0
    rv_med = df['realized_vol_24h'].median()
    df['realized_vol_24h'] = df['realized_vol_24h'].fillna(rv_med)

    # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ-–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç: "—Å–∫–æ–ª—å–∫–æ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–µ–π" –ø—Ä–æ—à–ª–∏
    df['pct_change_vol_norm'] = df['pct_change'] / (df['realized_vol_24h'] + 1e-8)

    return df

def add_realized_vol_if_missing(df: pd.DataFrame) -> pd.DataFrame:
    """
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –Ω–∞–ª–∏—á–∏–µ realized_vol_24h (–≤ %),
    –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ—Ç –∂–µ –ø–æ–¥—Ö–æ–¥, —á—Ç–æ –∏ –≤ create_regression_target.
    """
    df = df.copy()
    if 'realized_vol_24h' not in df.columns:
        if 'return' not in df.columns:
            df['return'] = df['close'].pct_change()
        df['realized_vol_24h'] = df['return'].rolling(24).std() * 100.0
        rv_med = df['realized_vol_24h'].median()
        df['realized_vol_24h'] = df['realized_vol_24h'].fillna(rv_med)
    return df


def add_vol_regime_label(
    df: pd.DataFrame,
    vol_col: str = 'realized_vol_24h',
    low_quantile: float = 0.33,
    high_quantile: float = 0.66,
) -> pd.DataFrame:
    """
    –°—Ç—Ä–æ–∏—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ realized_vol_24h.
    –≠—Ç–æ –ù–ï –ª–æ–º–∞–µ—Ç —Ñ–∏—á–∏ –º–æ–¥–µ–ª–∏ (–º—ã –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü vol_regime_label).
    """
    df = add_realized_vol_if_missing(df).copy()

    q_low = df[vol_col].quantile(low_quantile)
    q_high = df[vol_col].quantile(high_quantile)

    def _label(v: float) -> str:
        if v <= q_low:
            return 'low_vol'
        elif v >= q_high:
            return 'high_vol'
        else:
            return 'normal_vol'

    df['vol_regime_label'] = df[vol_col].apply(_label)
    return df


def build_feature_pipeline(
    df: pd.DataFrame,
    mode: str = "train",
    use_onchain: bool = True,
    use_macro: bool = True,
    use_trends: bool = True,
    use_derivatives: bool = True,
    use_orderbook_live: bool = False,
) -> pd.DataFrame:
    """
    –ï–¥–∏–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Ñ–∏—á–µ–π –¥–ª—è train/backtest/live.

    mode:
      - "train": –≤—Å—ë –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–±–µ–∑ LIVE orderbook)
      - "backtest": –∞–Ω–∞–ª–æ–≥ train, –Ω–æ –±–µ–∑ live-–∑–∞–ø—Ä–æ—Å–æ–≤
      - "live": —Ç–µ –∂–µ —Ñ–∏—á–∏ + LIVE orderbook (–µ—Å–ª–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ)
    """
    df = df.copy()

    # 1. –ë–∞–∑–æ–≤—ã–µ —Ü–µ–Ω–æ–≤—ã–µ —Ñ–∏—á–∏
    df = add_technical_indicators(df)
    df = add_multiscale_features(df)
    df = add_temporal_features(df)
    df = filter_anomalies(df)

    # 2. On-chain
    if use_onchain:
        df = add_onchain_features(df)

    # 3. –ú–∞–∫—Ä–æ + —Å–≤—è–∑–∞–Ω–Ω—ã–µ –≤–µ—â–∏
    if use_macro:
        df = add_macro_features(df)
        df = add_fear_greed_index(df)
        df = add_btc_dominance(df)

        if use_trends:
            df = add_google_trends(df)

        df = add_fed_rate(df)
        df = add_additional_macro(df)
        df = add_correlations_and_external(df)

    # 4. –î–µ—Ä–∏–≤–∞—Ç–∏–≤—ã
    if use_derivatives:
        df = add_derivatives_features(df)

    # 5. LIVE orderbook ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è live
    if mode == "live" and use_orderbook_live:
        df = add_orderbook_features_live(df)

    # 6. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å + —Ä–µ–∂–∏–º (–∫–∞—Ç–µ–≥–æ—Ä–∏—è)
    df = add_realized_vol_if_missing(df)
    df = add_vol_regime_label(df)

    return df